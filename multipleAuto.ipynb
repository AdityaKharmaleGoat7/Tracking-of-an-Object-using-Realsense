{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9469d6-c603-4bb4-bf4d-e46d16b03b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Initialize the RealSense pipeline\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)  # Enable depth stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Load YOLO model and classes\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Create a tracker object\n",
    "tracker = cv2.TrackerCSRT_create()\n",
    "\n",
    "# Initialize variables for tracking\n",
    "tracked_objects = []\n",
    "\n",
    "# Initialize a dictionary to store the last known position of tracked objects\n",
    "last_known_positions = {}\n",
    "\n",
    "while True:\n",
    "    # Wait for the next set of frames\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    depth_frame = frames.get_depth_frame()  # Retrieve depth frame\n",
    "    \n",
    "    if color_frame and depth_frame:\n",
    "        # Convert the color frame to a numpy array\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Detect objects using YOLO\n",
    "        blob = cv2.dnn.blobFromImage(color_image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(output_layers)\n",
    "\n",
    "        # Process YOLO output\n",
    "        boxes = []\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "                    # Object detected\n",
    "                    center_x = int(detection[0] * color_image.shape[1])\n",
    "                    center_y = int(detection[1] * color_image.shape[0])\n",
    "                    w = int(detection[2] * color_image.shape[1])\n",
    "                    h = int(detection[3] * color_image.shape[0])\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    boxes.append((x, y, w, h))\n",
    "\n",
    "        # Update tracking for each detected object\n",
    "        for box in boxes:\n",
    "            # Initialize tracker for new objects\n",
    "            tracker = cv2.TrackerCSRT_create()\n",
    "            tracker.init(color_image, box)\n",
    "            tracked_objects.append(tracker)\n",
    "\n",
    "        # Update and draw tracked objects\n",
    "        for tracker in tracked_objects:\n",
    "            ok, bbox = tracker.update(color_image)\n",
    "            if ok:\n",
    "                # Tracking successful, draw bounding box\n",
    "                p1 = (int(bbox[0]), int(bbox[1]))\n",
    "                p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "                cv2.rectangle(color_image, p1, p2, (0, 255, 0), 2)\n",
    "                # Store the last known position of the tracked object\n",
    "                last_known_positions[tracker] = (p1, p2)\n",
    "            else:\n",
    "                # Tracking failed, remove tracker\n",
    "                tracked_objects.remove(tracker)\n",
    "\n",
    "        # Filter out overlapping bounding boxes\n",
    "        keys_to_remove = []\n",
    "        for tracker1, bbox1 in last_known_positions.items():\n",
    "            for tracker2, bbox2 in last_known_positions.items():\n",
    "                if tracker1 != tracker2 and bbox1[0][0] <= bbox2[1][0] and bbox1[1][0] >= bbox2[0][0] \\\n",
    "                        and bbox1[0][1] <= bbox2[1][1] and bbox1[1][1] >= bbox2[0][1]:\n",
    "                    # Bounding boxes overlap, add the key to the removal list\n",
    "                    keys_to_remove.append(tracker2)\n",
    "\n",
    "        # Remove keys from the dictionary\n",
    "        for key in keys_to_remove:\n",
    "            if key in last_known_positions:\n",
    "                del last_known_positions[key]\n",
    "\n",
    "        # Display the color image\n",
    "        cv2.imshow('Object Tracking', color_image)\n",
    "\n",
    "    # Check for key press\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:\n",
    "        # Exit program when 'ESC' is pressed\n",
    "        break\n",
    "\n",
    "# Stop the pipeline and close all OpenCV windows\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5643d798-99e5-41d8-8467-7aa11fb76fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Initialize the RealSense pipeline\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)  # Enable depth stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Load YOLO model and classes\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Create a tracker object\n",
    "tracker = cv2.TrackerCSRT_create()\n",
    "\n",
    "# Initialize variables for tracking\n",
    "tracked_objects = []\n",
    "\n",
    "# Initialize a dictionary to store the last known position of tracked objects\n",
    "last_known_positions = {}\n",
    "\n",
    "while True:\n",
    "    # Wait for the next set of frames\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    depth_frame = frames.get_depth_frame()  # Retrieve depth frame\n",
    "    \n",
    "    if color_frame and depth_frame:\n",
    "        # Convert the color frame to a numpy array\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Resize the image for faster processing\n",
    "        resized_image = cv2.resize(color_image, (416, 416))\n",
    "\n",
    "        # Detect objects using YOLO\n",
    "        blob = cv2.dnn.blobFromImage(resized_image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(output_layers)\n",
    "\n",
    "        # Process YOLO output\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "                    # Object detected\n",
    "                    center_x = int(detection[0] * color_image.shape[1])\n",
    "                    center_y = int(detection[1] * color_image.shape[0])\n",
    "                    w = int(detection[2] * color_image.shape[1])\n",
    "                    h = int(detection[3] * color_image.shape[0])\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    boxes.append((x, y, w, h))\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Apply non-max suppression to remove overlapping bounding boxes\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        # Update tracking for each detected object\n",
    "        if len(indices) > 0:\n",
    "            for i in indices.flatten():\n",
    "                box = boxes[i]\n",
    "                x, y, w, h = box\n",
    "\n",
    "                # Initialize tracker for new objects\n",
    "                tracker = cv2.TrackerCSRT_create()\n",
    "                tracker.init(color_image, (x, y, w, h))\n",
    "                tracked_objects.append(tracker)\n",
    "\n",
    "        # Update and draw tracked objects\n",
    "        for tracker in tracked_objects:\n",
    "            ok, bbox = tracker.update(color_image)\n",
    "            if ok:\n",
    "                # Tracking successful, draw bounding box\n",
    "                p1 = (int(bbox[0]), int(bbox[1]))\n",
    "                p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "                cv2.rectangle(color_image, p1, p2, (0, 255, 0), 2)\n",
    "                # Store the last known position of the tracked object\n",
    "                last_known_positions[tracker] = (p1, p2)\n",
    "            else:\n",
    "                # Tracking failed, remove tracker\n",
    "                tracked_objects.remove(tracker)\n",
    "\n",
    "        # Display the color image\n",
    "        cv2.imshow('Object Tracking', color_image)\n",
    "\n",
    "    # Check for key press\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:\n",
    "        # Exit program when 'ESC' is pressed\n",
    "        break\n",
    "\n",
    "# Stop the pipeline and close all OpenCV windows\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22c455-35fe-43d4-85bf-c43783e417af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
